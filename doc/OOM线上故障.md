一、问题现象
9月2号上午业务突然反馈OMS系统异常，页面提示“服务未找到”，登入k8s查看ecoflow-prm服务，发现两台服务都已宕机，导致无法访问此服务。紧急重启prm服务后系统恢复正常。
[图片]

二、问题分析
查看日志确认是内存溢出（OOM）问题，但具体是哪段代码导致的问题需要花时间分析。由于上周五正常迭代发版，此前没有发生OOM问题，基本上确认是本次发版导致的问题，检查了一下发版的代码，发现代码没什么问题。
OOM与慢查询有着密切的关系，当数据库查询执行时间过长，可能是由于单表查询或联表查询结果数据量很大，加载到内存中，没有及时释放，导致内存不断累加，最终触发OOM。另外，慢sql查询时间过长，可能会阻塞其它查询，数据库可能会因为处理请求累积过多，耗尽了内存资源，从而导致了OOM。
因此，排查日志从慢sql入手，发现查询prm_open_order表几乎进行了全表查询，而prm_open_order表上周五上线后由于处理千易历史数据，此表增加了四千多条数据，而且prm_open_order表存储了原接口的订单Json数据，每一条数据在1k左右。此查询方法是在看板数据统计接口中调用的，和前端了解后，如果在浏览器打开OMS系统进入首页，就会每隔一分钟调用一次统计接口。
基本上可以怀疑是查询prm_open_order表数据过大，查询频率过高导致内存没有及时释放最终导致OOM。


三、解决方案
1. 后端代码优化。看板统计数据接口查询prm_open_order表只查询所需要的交易路线、sap单号字段，不需要将整表所有字段一次性查出。
2. 前端调用数据统计接口频次优化。
   代码更新后上线，继续观察是否还会再出现OOM。


四、后续优化点
在日志排查过程中，发现查询日志比较耗时，服务容灾能力较弱等问题，后续需要优化的点有：
1. 服务需要加监控，线上OOM能及时预警，发生了OOM能自动导出dump文件，方便问题排查
2. 接口自动重试，如果一台服务宕机，调用此服务失败后会继续重试调用其它服务
3. k8s的pod内存设置为8G，默认的JVM为其1/4，即2G，需要手动设置JVM内存的大小
4. 业务规范：数据库中有大字段的查询，非必要不要把大字段查询出来。